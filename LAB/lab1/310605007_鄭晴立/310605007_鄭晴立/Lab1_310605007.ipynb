{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class optimizer():\n",
    "    def __init__(self, name = 'SGD', lr = 0.001, hyper_parameter = {}):\n",
    "        self.name = name\n",
    "        self.lr = lr\n",
    "        self.hyper_parameter = hyper_parameter\n",
    "\n",
    "\n",
    "class FC_layer():\n",
    "    def __init__(self, input_size=16, output_size=16, bias=True):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.bias = bias\n",
    "\n",
    "        self.weight_matrix = np.random.uniform(-1, 1, (input_size, output_size))\n",
    "        if self.bias:\n",
    "            self.bias_matrix = np.random.uniform(-1, 1, (1, output_size))\n",
    "\n",
    "        self.trained = 0\n",
    "        \n",
    "    def forward(self, x:np.array):\n",
    "        self.inputs = x\n",
    "        self.batch_size = x.shape[0]\n",
    "        if self.bias:\n",
    "            return np.dot(x, self.weight_matrix) + self.bias_matrix\n",
    "        else:\n",
    "            return np.dot(x, self.weight_matrix)\n",
    "\n",
    "    def backward(self, dy:np.array, opti:optimizer):\n",
    "        dx = np.dot(dy, np.transpose(self.weight_matrix))\n",
    "        \n",
    "        lr = opti.lr\n",
    "        dw = np.dot(np.transpose(self.inputs), dy) / self.batch_size\n",
    "        if self.bias:\n",
    "            db = np.sum(dy, axis=0) / self.batch_size\n",
    "            \n",
    "\n",
    "        if opti.name == 'SGD':\n",
    "            self.weight_matrix = self.weight_matrix - lr * dw\n",
    "            if self.bias:\n",
    "                self.bias_matrix = self.bias_matrix - lr * db\n",
    "        \n",
    "        elif opti.name == 'momentum':\n",
    "            beta = opti.hyper_parameter['beta']\n",
    "            if self.trained ==  0:\n",
    "                self.velocity = np.zeros((self.input_size, self.output_size))\n",
    "                if self.bias:\n",
    "                    self.velocity_b = np.zeros((1, self.output_size))\n",
    "\n",
    "            self.velocity = beta * self.velocity - lr * dw\n",
    "            self.weight_matrix = self.weight_matrix + self.velocity\n",
    "            if self.bias:\n",
    "                self.velocity_b = beta * self.velocity_b - lr * db\n",
    "                self.bias_matrix = self.bias_matrix + self.velocity_b\n",
    "\n",
    "        elif opti.name == 'adagrad':\n",
    "            epsilon = opti.hyper_parameter['epsilon']\n",
    "            if self.trained == 0:\n",
    "                self.v = np.zeros((self.input_size, self.output_size))\n",
    "                if self.bias:\n",
    "                    self.v_b = np.zeros((1, self.output_size))\n",
    "            \n",
    "            self.v = self.v + (dw ** 2)\n",
    "            self.weight_matrix = self.weight_matrix - lr * dw / np.sqrt(self.v + epsilon)\n",
    "            if self.bias:\n",
    "                self.v_b = self.v_b + (db ** 2)\n",
    "                self.bias_matrix = self.bias_matrix - lr * db / np.sqrt(self.v_b + epsilon)      \n",
    "            \n",
    "        elif opti.name == 'adam':\n",
    "            epsilon = opti.hyper_parameter['epsilon']\n",
    "            beta1 = opti.hyper_parameter['beta1']\n",
    "            beta2 = opti.hyper_parameter['beta2']\n",
    "            if self.trained == 0:\n",
    "                self.m = np.zeros((self.input_size, self.output_size))\n",
    "                self.v = np.zeros((self.input_size, self.output_size))\n",
    "                if self.bias:\n",
    "                    self.m_b = np.zeros((1, self.output_size))\n",
    "                    self.v_b = np.zeros((1, self.output_size))\n",
    "            \n",
    "            self.m = beta1 * self.m + (1 - beta1) * dw\n",
    "            self.v = beta2 * self.v + (1 - beta2) * (dw ** 2)\n",
    "            m_hat = self.m / (1 - beta1)\n",
    "            v_hat = self.v / (1 - beta2)\n",
    "            self.weight_matrix = self.weight_matrix - lr * m_hat  / np.sqrt(v_hat + epsilon)\n",
    "            \n",
    "            if self.bias:\n",
    "                self.m_b = beta1 * self.m_b + (1 - beta1) * db\n",
    "                self.v_b = beta2 * self.v_b + (1 - beta2) * (db ** 2)\n",
    "                m_b_hat = self.m_b / (1 - beta1)\n",
    "                v_b_hat = self.v_b / (1 - beta2)\n",
    "                self.bias_matrix = self.bias_matrix - lr * m_b_hat  / np.sqrt(v_b_hat + epsilon)\n",
    "        \n",
    "        else: #SGD\n",
    "            self.weight_matrix = self.weight_matrix - lr * dw\n",
    "            if self.bias:\n",
    "                self.bias_matrix = self.bias_matrix - lr * db\n",
    "\n",
    "        self.trained = 1\n",
    "        return dx\n",
    "\n",
    "\n",
    "class relu():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x:np.array):\n",
    "        self.inputs = x\n",
    "        return self.relu(x)\n",
    "    \n",
    "    def relu(self, x:np.array):\n",
    "        return np.maximum(x, 0)\n",
    "\n",
    "    def backward(self, dy:np.array, opti:optimizer):\n",
    "        dx = (self.inputs > 0) * dy\n",
    "        return dx\n",
    "\n",
    "\n",
    "class sigmoid():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x:np.array):\n",
    "        self.inputs = x\n",
    "        return self.sigmoid(x)\n",
    "    \n",
    "    def sigmoid(self, x:np.array):\n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "    def backward(self, dy:np.array, opti:optimizer):\n",
    "        dx = self.sigmoid(self.inputs) * (1 - self.sigmoid(self.inputs)) * dy\n",
    "        return dx\n",
    "\n",
    "\n",
    "class myNN():\n",
    "    def __init__(self, layers=[]):\n",
    "        self.layers = layers\n",
    "\n",
    "    def forward(self, x:np.array):\n",
    "        data = x\n",
    "        for layer in self.layers:\n",
    "            data = layer.forward(data)\n",
    "        return data\n",
    "\n",
    "    def backward(self, dy:np.array, opti:optimizer):\n",
    "        self.layers.reverse()\n",
    "        for layer in self.layers:\n",
    "            dy = layer.backward(dy, opti)\n",
    "        self.layers.reverse()\n",
    "\n",
    "\n",
    "class MSE():\n",
    "    def __init__(self):\n",
    "        self.size = 0\n",
    "    \n",
    "    def forward(self, y:np.array, y_pred:np.array):\n",
    "        self.size = y.shape[0]\n",
    "        loss = (y - y_pred) ** 2\n",
    "        loss = np.sum(loss) / self.size\n",
    "        return loss\n",
    "\n",
    "    def backward(self:np.array, y, y_pred:np.array):\n",
    "        dy = 2 * (y_pred - y)\n",
    "        return dy\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "2bb53744726dcb34925a52c17673bca24c61f3e23ffd232b95f4973e181ddeaa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
