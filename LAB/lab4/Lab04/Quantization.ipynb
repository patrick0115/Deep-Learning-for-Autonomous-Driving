{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from speech_command_dataset import SpeechCommandDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from model import M5\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True \n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare dataloader\n",
    "calib_params = {\"batch_size\": ,\n",
    "                \"shuffle\": True,\n",
    "                \"drop_last\": True,\n",
    "                \"num_workers\": 1}\n",
    "\n",
    "testing_params = {\"batch_size\": ,\n",
    "                       \"shuffle\": False,\n",
    "                       \"drop_last\": True,\n",
    "                       \"num_workers\": 1}\n",
    "\n",
    "calib_set = SpeechCommandDataset()\n",
    "calib_loader = DataLoader(calib_set, **calib_params)\n",
    "\n",
    "test_set = SpeechCommandDataset(is_training=False)\n",
    "test_loader = DataLoader(test_set, **testing_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, epoch):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        #forward\n",
    "        output = model(data)\n",
    "\n",
    "        pred = output.argmax(dim=-1)\n",
    "        correct += pred.squeeze().eq(target).sum().item()\n",
    "        \n",
    "    # print testing stats\n",
    "    test_acc = 100.0 * float(correct) / len(test_set)\n",
    "    print('Epoch: %3d' % epoch, '|test accuracy: %.2f' % test_acc)\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model_path = './Checkpoint/best_model.pth.tar'\n",
    "\n",
    "print(\"=> loading checkpoint '{}'\".format(model_path))\n",
    "checkpoint = torch.load(model_path, map_location = device)\n",
    "\n",
    "model = M5(cfg = checkpoint['cfg']).to(device)\n",
    "model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nbytes per element:', model.features[0].weight.element_size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static quantization of a model consists of the following steps:\n",
    "\n",
    "1. Fuse modules\n",
    "2. Insert Quant/DeQuant Stubs\n",
    "3. Prepare the fused module (insert observers before and after layers)\n",
    "4. Calibrate the prepared module (pass it representative data)\n",
    "5. Convert the calibrated module (replace with quantized version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Fuse modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "_ = torch.quantization.fuse_modules(model.features, ['0','1','2'], inplace=True)\n",
    "_ = torch.quantization.fuse_modules(model.features, ['4','5','6'], inplace=True)\n",
    "_ = torch.quantization.fuse_modules(model.features, ['8','9','10'], inplace=True)\n",
    "_ = torch.quantization.fuse_modules(model.features, ['12','13','14'], inplace=True)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Insert Quant/DeQuant Stubs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Insert stubs\"\"\"\n",
    "model = nn.Sequential(torch.quantization.QuantStub(), \n",
    "                  *model.features,\n",
    "                   model.avgpool,\n",
    "                   model.flatten,\n",
    "                   model.fc,\n",
    "                   torch.quantization.DeQuantStub())\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prepare the fused module (insert observers before and after layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backend = \"fbgemm\"  # running on a x86 CPU. Use \"qnnpack\" if running on ARM.\n",
    "\n",
    "model.qconfig = torch.quantization.get_default_qconfig(backend)\n",
    "torch.quantization.prepare(model, inplace=True)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Calibrate the prepared module (pass it representative data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(calib_loader)\n",
    "\n",
    "NUM_CALIB_BATCH = 10\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for _ in range(NUM_CALIB_BATCH):\n",
    "        inputs, labels = next(iterator)\n",
    "        inputs = inputs.cpu()\n",
    "        labels = labels.cpu()\n",
    "        outputs = model(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Convert the calibrated module (replace with quantized version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Convert\"\"\"\n",
    "quantized_model = torch.quantization.convert(model, inplace=False)\n",
    "\n",
    "print(quantized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nbytes per element:', quantized_model[1].weight().element_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = test(quantized_model, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "best_path = './Checkpoint/best_model.pth.tar'\n",
    "fine_path = './Checkpoint/fine_model.pth.tar'\n",
    "coarse_path = './Checkpoint/coarse_model.pth.tar'\n",
    "\n",
    "best_checkpoint = torch.load(best_path, map_location = device)\n",
    "best_model = M5(cfg = best_checkpoint['cfg']).to(device)\n",
    "best_model.load_state_dict(best_checkpoint['state_dict'])\n",
    "\n",
    "fine_checkpoint = torch.load(fine_path, map_location = device)\n",
    "fine_model = M5(cfg = fine_checkpoint['cfg']).to(device)\n",
    "fine_model.load_state_dict(fine_checkpoint['state_dict'])\n",
    "\n",
    "coarse_checkpoint = torch.load(coarse_path, map_location = device)\n",
    "coarse_model = M5(cfg = coarse_checkpoint['cfg']).to(device)\n",
    "coarse_model.load_state_dict(coarse_checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_benchmark(model, num_batch):\n",
    "    model.eval()\n",
    "    elapsed = 0\n",
    "    \n",
    "    for i, (data, target) in enumerate(test_loader):\n",
    "\n",
    "        data = data.to(device)\n",
    "        #forward\n",
    "        start = time.perf_counter()\n",
    "        output = model(data)\n",
    "        end = time.perf_counter()\n",
    "        elapsed = elapsed + (end-start)\n",
    "        \n",
    "        if i == num_batch-1:\n",
    "            break\n",
    "    print('inference time: %.3f s' % (elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_BATCH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_benchmark(quantized_model, NUM_BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_benchmark(best_model, NUM_BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_benchmark(fine_model, NUM_BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_benchmark(coarse_model, NUM_BATCH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
